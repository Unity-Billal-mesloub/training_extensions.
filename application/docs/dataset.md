# Dataset management

## Overview

In Geti Tune, the _dataset_ is the container for all the media and related annotations within a project.
New items can be added to the dataset from:

- the inference pipeline, whose data collection stage implements policies to select which frames to save,
  along with the predictions generated by the model.
- the user, who manually uploads images through the web interface or the REST API.

The dataset, uniquely associated with a project, serves as input for various analytical workflows such as model
fine-tuning, evaluation, and data exploration. The user has full control over the dataset, being able to add and remove
elements, edit annotations and other attributes.

### Data flow

The following diagram shows the flow of dataset-related data, from the origin to where they are stored and consumed.

![Diagram showing the dataset manager](media/dataset-management.jpg)

The metadata relative to the dataset items, including the annotations, is stored in a database (SQLite), while the
actual media files are stored in the filesystem. The dataset items can be accessed and manipulated via REST API,
with little to no conversion since both the API and the DB use the same JSON schema to represent annotations.

The inference stage of the pipeline, followed by the dataset collection module, streams newly acquired media items
(images represented as `np.array`) along with their predictions (a subclass of `model_api.Result`) and other metadata.
Before storing them in the database, they must be first converted to the internal representation used by Geti Tune.
Note that the conversion is only one way, as there is no need to convert back to the `model_api` data structures.

Workflows like training, evaluation and dataset export expect a `datumaro.Dataset` object as input, which can be
built on-the-fly through a converter from the DB records to `datumaro.Sample` objects.

It is important to observe that the choice of a database as a backend to store the dataset items is justified by the
various transactional use cases of dataset management, such as appending a new item or updating an existing annotation.
Conversely, heavy processes like training and evaluation benefit from `datumaro` data structures, since they are
specifically designed for analytical workloads that consume data in batches.
Note that Datumaro also allows to export datasets in a variety of formats, useful for interoperability with other tools.

## Dataset views

A dataset view in Geti Tune is a user-defined subset of items within a dataset, designed to facilitate the organization
and exploration of data. Rather than working with the entire dataset, users can create views that include only the items
relevant to a specific task, for example exporting the items collected in the last week, or evaluating the model on
a specific subset of media. Especially when datasets grow large or contain diverse data, users can filter the items
based on properties such as labels, timestamps and tags, then assign the resulting subset to a view for easier access.

Dataset views are flexible and fully controlled by the user. Items can be added to or removed from a view either
manually or by applying filters that match certain criteria. For example, a view might consist of all images tagged
as "rainy weather" or collected from a specific production site.

Importantly, views do not duplicate data; they reference the items already present in the main dataset.
This ensures that storage remains efficient and that any updates to the underlying items are reflected in all views
where those items appear. Removing an item from a view does not delete it from the dataset; conversely, deleting an item
from the dataset automatically removes it from all views.

## Dataset revisions

A dataset revision in Geti Tune is an immutable snapshot of a dataset (or dataset view), capturing its exact state at
a specific point in time.
This mechanism is essential for ensuring traceability and reproducibility in machine learning workflows,
particularly when training or evaluating models. When a user initiates a training or evaluation job, Geti Tune
creates a dataset revision that includes all the items, annotations, and metadata as they existed at that moment.

Dataset revisions are tightly integrated with model management. Each model revision references the dataset revision
it was trained on, allowing users to trace back and audit the exact data that contributed to a modelâ€™s performance.
It allows not only to reproduce the training process in another environment, which is useful to diagnose bugs,
but also to train other models with different architecture/configuration on the same data and compare the results.

Dataset revisions are stored separately from the main dataset, typically in a read-only format such as Apache Arrow,
the same used by the underlying training pipeline. Therefore, the revision is optimized for efficient access during
training, and being a file it can be easily exported and shared.
Subsequent changes to the main dataset do not affect the revision, preserving the integrity of the data used for the
operation. In other words, if the user removes a media or edits an annotation, that has no effect on the existing
revisions, which store their own copy of the items.
Since the revisions may consume significant storage space, users can choose to delete them when they are no longer
needed; in this case, a small record is kept in the database with essential metadata (e.g. creation time, number of
items), but the actual items are removed and can no longer be accessed.

## Storage

### DB schema

![Diagram showing the DB tables](media/storage/datasets.svg)

The database saves the relevant information in the following tables:

- `dataset_items`: contains one record for each dataset item, identified by a unique id. The record contains
  metadata about the media (filename, format, size, shape), the annotation data in JSON format (more details
  [here](#annotations-as-json) and information about the subset assignment (training, validation or testing).
  It also includes a reference to the source (e.g. a camera) from which the media was acquired;
  the field can be null if the media was uploaded manually.
  The distinction between annotations and predictions is determined by the fields `from_model` and `user_reviewed`
  (more details [here](#annotations-vs-predictions))).
  The actual media file is stored in the filesystem with a name that matches the id and the format
  (e.g. `e4531145-540f-4a0b-8e70-02a70aef9637.jpg`); the same applies to the corresponding thumbnail image.
- `dataset_views`: one record for each view, identified by a unique id and a name.
- `dataset_view_items`: a many-to-many relationship table that associates dataset items with views. Each record contains
  the id of the dataset item and the id of the view it belongs to.
- `dataset_revisions`: simple metadata about each revision, such as creation time and number of items. The actual data
  is instead stored in a file on the filesystem.
- `dataset_items_tags`: a many-to-many relationship table that associates tags with dataset items. Each record contains
  the id of the dataset item and the the tag as a string.

### Details

#### Annotations schema

The schema of the `annotations.data` field is flexible and can represent different types of annotations.
It is a JSON list of objects, where each object represents a single annotation or prediction and it consists of a
shape and one or more labels. The shape defines the geometry of the annotation, while the labels provide additional
information about the object(s) in the image, such as their class or confidence score. The shape schema depends on the
type of annotation, while the labels schema is consistent across all types.

```json
[
  {
    "labels": [{"id": "d476573e-d43c-42a6-9327-199a9aa75c33", "confidence": 0.7}],
    "shape": <shape_1>
  },
  {
    "labels": [{"id": "d622bfdd-83a9-49c6-9f8d-e58019b2f553", "confidence": 0.9}],
    "shape": <shape_2>
  }
]
```

**Classification**

For classification tasks, the shape is a full image, meaning that the annotation applies to the whole image.

```json
{ "shape": { "type": "full_image" } }
```

**Detection**

For detection tasks, the shape is a bounding box defined by its top-left corner coordinates, width and height.
All coordinates are absolute, integer, and must be within the image bounds.

```json
{
  "shape": {
    "type": "rectangle",
    "x": 10,
    "y": 20,
    "width": 100,
    "height": 200
  }
}
```

**Segmentation**

For segmentation tasks, the shape is a polygon defined by a list of points (x, y) that form the vertices of the polygon.
The points are absolute, integer, and must be within the image bounds. The polygon can be concave or convex but must
not be self-intersecting. The points are ordered in a counter-clockwise manner.

```json
{
  "shape": {
    "type": "polygon",
    "points": [
      [10, 20],
      [20, 60],
      [30, 40]
    ]
  }
}
```

#### Annotations as JSON

The choice of storing annotations as JSON is motivated by the need to support different types of annotations while
keeping the database schema simple and flexible. Moreover, the same JSON representation is also used by the REST API,
effectively making it easy and fast to access annotations via API. The downside is that advanced dataset filter queries
based on the annotation properties (e.g. min annotation size) are more difficult to implement directly in the database,
even though it is still possible thanks to SQLite JSON functions like `json_extract()`.

#### Annotations vs predictions

The fields `from_model` and `user_reviewed` together define the source of the annotation and its status:

- `from_model` is a UUID that identifies the model that generated the prediction. If the annotation was created
  manually by the user, this field is null. When the user accepts a prediction _without making changes_, the value of this
  field is kept unchanged, else it is reset to null.
- `user_reviewed` is a boolean that indicates whether the user has confirmed the correctness of the annotation,
  explicitly (by accepting a prediction) or implicitly (when creating or editing the annotation).

The following table summarizes the possible combinations of these two fields and their meaning:

| `from_model` | `user_reviewed` | meaning                                                                |
| ------------ | --------------- | ---------------------------------------------------------------------- |
| <model_id>   | false           | Prediction not reviewed yet                                            |
| <model_id>   | true            | Prediction reviewed and accepted as it is                              |
| null         | false           | Not possible                                                           |
| null         | true            | Annotation created by the user, from scratch or modifying a prediction |

## Services

A couple of service classes, `DatasetService` and `AnnotationService`, encapsulate the logic for managing
dataset items and their annotations. They provide methods to create, read, update and delete these resources,
as well as to perform more complex operations like conversions between formats (e.g. model_api -> datumaro).

Examples of CRUD-like methods:

- `DatasetService.add_media_to_dataset`
- `AnnotationService.update_annotation`

Examples of more complex methods:

- `DatasetService.get_item_from_mapi_result`
- `DatasetService.get_dataset_as_datumaro`

## REST API

See the [API reference](api.md).
